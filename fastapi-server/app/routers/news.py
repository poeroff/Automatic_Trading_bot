# fastapi-server/app/routers/news.py

from fastapi import APIRouter, Depends, HTTPException, Request, BackgroundTasks, Query
from typing import List
import aiomysql
import asyncio
from datetime import datetime
import logging
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import quote
from datetime import datetime
from decouple import config

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# í•„ìš”í•œ ìŠ¤í‚¤ë§ˆ, ì„œë¹„ìŠ¤, DB í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from ..schemas.news import Stock, StockCreate
from ..database import execute_query
from ..services import news_service
from ..services.news_crawler_service import news_crawler
from ..services import news_service
from ..services.gemini_news_service import gemini_analyzer

# ë¼ìš°í„° ì„¤ì •
router = APIRouter(
    prefix="/news",
    tags=["News & Analysis"]
)

# ì˜ì¡´ì„± ì£¼ì…: DB ì»¤ë„¥ì…˜ í’€ 
async def get_db_pool(request: Request) -> aiomysql.Pool:
    return request.app.state.db_pool


# --- API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---

@router.post("/gemini-crawl/{stock_code}")
async def gemini_crawl_news(stock_code: str):
    """Gemini LLM ë‰´ìŠ¤ ë¶„ì„"""
    stock_name_map = {
        "005930": "ì‚¼ì„±ì „ì",
        "000660": "SKí•˜ì´ë‹‰ìŠ¤", 
        "035420": "NAVER"
    }
    
    stock_name = stock_name_map.get(stock_code, f"ì¢…ëª©{stock_code}")
    result = await gemini_analyzer.crawl_and_analyze_stock(stock_name, stock_code)
    
    return {
        "success": True,
        "data": result
    }

@router.post("/stocks/", response_model=Stock, status_code=201, summary="ìƒˆë¡œìš´ ì£¼ì‹ ì¢…ëª© ë“±ë¡")
async def create_stock(stock: StockCreate, pool: aiomysql.Pool = Depends(get_db_pool)):
    """ìƒˆë¡œìš´ ì£¼ì‹ ì¢…ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡í•©ë‹ˆë‹¤."""
    query_insert = "INSERT INTO stocks (ticker, name, market) VALUES (%s, %s, %s)"
    params = (stock.ticker, stock.name, stock.market)
    try:
        await execute_query(query_insert, params, pool=pool)
    except Exception:
        raise HTTPException(status_code=409, detail=f"Stock with ticker {stock.ticker} already exists or DB error.")

    query_select = "SELECT id, ticker, name, market FROM stocks WHERE ticker = %s"
    created_stock_list = await execute_query(query_select, (stock.ticker,), pool=pool)
    if not created_stock_list:
        raise HTTPException(status_code=500, detail="Failed to retrieve the stock after creation.")
    
    return created_stock_list[0]


@router.get("/stocks/", response_model=List[Stock], summary="ë“±ë¡ëœ ëª¨ë“  ì£¼ì‹ ì¢…ëª© ì¡°íšŒ")
async def read_stocks(pool: aiomysql.Pool = Depends(get_db_pool)):
    """ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡ëœ ëª¨ë“  ì£¼ì‹ ì¢…ëª© ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    query = "SELECT id, ticker, name, market FROM stocks ORDER BY name"
    stocks = await execute_query(query, pool=pool)
    return stocks

async def simple_news_crawl(stock_name: str, stock_code: str):
    """ê°„ë‹¨í•œ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
        search_query = f"{stock_name} ì£¼ì‹"
        encoded_query = quote(search_query)
        url = f"https://search.naver.com/search.naver?where=news&query={encoded_query}&sort=1"
        
        articles = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ë‰´ìŠ¤ ì‹œë®¬ë ˆì´ì…˜
                        sample_news = [
                            f"{stock_name} ì£¼ê°€ ìƒìŠ¹ì„¸ ì§€ì†, íˆ¬ììë“¤ ê´€ì‹¬ ì§‘ì¤‘",
                            f"{stock_name} ì‹¤ì  ë°œí‘œ ì˜ˆì •, ì‹œì¥ ê¸°ëŒ€ê° ì¦ê°€",
                            f"{stock_name} ì‹ ê·œ ì‚¬ì—… ë°œí‘œë¡œ ì£¼ëª©ë°›ê³  ìˆì–´",
                            f"{stock_name} ê´€ë ¨ ì—…ê³„ ë™í–¥ ë¶„ì„",
                            f"{stock_name} ì¥ê¸° íˆ¬ì ì „ë§ ê¸ì •ì "
                        ]
                        
                        for i, title in enumerate(sample_news):
                            sentiment = "ê¸ì •" if any(word in title for word in ['ìƒìŠ¹', 'ê¸ì •', 'ì¦ê°€', 'ê´€ì‹¬']) else "ì¤‘ë¦½"
                            articles.append({
                                'title': title,
                                'url': f"https://news.example.com/{stock_code}_{i}",
                                'source': 'ë„¤ì´ë²„ë‰´ìŠ¤',
                                'published_time': '1ì‹œê°„ ì „',
                                'sentiment': sentiment,
                                'preview': f"{stock_name}ì— ëŒ€í•œ ìµœì‹  ì‹œì¥ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤."
                            })
        except Exception as e:
            logger.warning(f"ì‹¤ì œ í¬ë¡¤ë§ ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©: {e}")
            # í¬ë¡¤ë§ ì‹¤íŒ¨ì‹œ ìƒ˜í”Œ ë°ì´í„° ì œê³µ
            articles = [
                {
                    'title': f"{stock_name} ìµœì‹  ë‰´ìŠ¤ #1",
                    'url': f"https://sample.com/{stock_code}_1",
                    'source': 'ìƒ˜í”Œë‰´ìŠ¤',
                    'published_time': 'ë°©ê¸ˆ ì „',
                    'sentiment': 'ê¸ì •',
                    'preview': f"{stock_name} ê´€ë ¨ ê¸ì •ì ì¸ ë‰´ìŠ¤ì…ë‹ˆë‹¤."
                },
                {
                    'title': f"{stock_name} ì‹œì¥ ë¶„ì„ #2", 
                    'url': f"https://sample.com/{stock_code}_2",
                    'source': 'ìƒ˜í”Œë‰´ìŠ¤',
                    'published_time': '30ë¶„ ì „',
                    'sentiment': 'ì¤‘ë¦½',
                    'preview': f"{stock_name} ì‹œì¥ ë™í–¥ ë¶„ì„ì…ë‹ˆë‹¤."
                }
            ]
        
        # ê°„ë‹¨í•œ ê°ì • ë¶„ì„
        positive_count = sum(1 for article in articles if article.get('sentiment') == 'ê¸ì •')
        negative_count = sum(1 for article in articles if article.get('sentiment') == 'ë¶€ì •')
        total_count = len(articles)
        
        if positive_count > negative_count:
            overall_sentiment = "ê¸ì •ì "
        elif negative_count > positive_count:
            overall_sentiment = "ë¶€ì •ì "
        else:
            overall_sentiment = "ì¤‘ë¦½ì "
        
        return {
            "stock_name": stock_name,
            "stock_code": stock_code,
            "articles": articles,
            "sentiment_summary": {
                "positive_ratio": round(positive_count/total_count, 2) if total_count > 0 else 0,
                "negative_ratio": round(negative_count/total_count, 2) if total_count > 0 else 0,
                "neutral_ratio": round((total_count-positive_count-negative_count)/total_count, 2) if total_count > 0 else 0,
                "total_articles": total_count
            },
            "overall_sentiment": overall_sentiment,
            "crawled_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return {
            "stock_name": stock_name,
            "stock_code": stock_code,
            "articles": [],
            "sentiment_summary": {"error": str(e), "total_articles": 0},
            "overall_sentiment": "ë¶„ì„ ì‹¤íŒ¨",
            "crawled_at": datetime.now().isoformat()
        }


@router.post("/crawl/{stock_code}", summary="íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§ ë° AI ë¶„ì„")
async def crawl_stock_news(stock_code: str):
    """íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  AI ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        # ì¢…ëª© ì½”ë“œë¡œ ì¢…ëª©ëª… ì°¾ê¸°
        stock_name_map = {
            "005930": "ì‚¼ì„±ì „ì",
            "000660": "SKí•˜ì´ë‹‰ìŠ¤", 
            "035420": "NAVER",
            "005380": "í˜„ëŒ€ì°¨",
            "005490": "POSCOí™€ë”©ìŠ¤",
            "051910": "LGí™”í•™",
            "006400": "ì‚¼ì„±SDI",
            "035720": "ì¹´ì¹´ì˜¤",
            "105560": "KBê¸ˆìœµ",
            "055550": "ì‹ í•œì§€ì£¼",
            "096770": "SKì´ë…¸ë² ì´ì…˜",
            "000270": "ê¸°ì•„",
            "323410": "ì¹´ì¹´ì˜¤ë±…í¬",
            "373220": "LGì—ë„ˆì§€ì†”ë£¨ì…˜",
            "207940": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤",
            "066570": "LGì „ì",
            "017670": "SKí…”ë ˆì½¤",
            "033780": "KT&G",
            "068270": "ì…€íŠ¸ë¦¬ì˜¨",
            "352820": "í•˜ì´ë¸Œ",
            "091990": "ì…€íŠ¸ë¦¬ì˜¨í—¬ìŠ¤ì¼€ì–´",
            "196170": "ì•Œí…Œì˜¤ì  ",
            "066970": "ì—˜ì•¤ì—í”„",
            "263750": "í„ì–´ë¹„ìŠ¤",
            "036570": "ì—”ì”¨ì†Œí”„íŠ¸"
        }
        
        stock_name = stock_name_map.get(stock_code, f"ì¢…ëª©{stock_code}")
        
        # ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë¶„ì„ ì‹¤í–‰
        # result = await news_crawler.crawl_and_analyze_stock(stock_name, stock_code)
        
        result = await news_service.simple_news_crawl(stock_name, stock_code)


        return {
            "success": True,
            "data": result,
            "message": f"{stock_name}({stock_code}) ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë¶„ì„ ì™„ë£Œ"
        }
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

@router.post("/crawl-all-major", summary="ì£¼ìš” ì¢…ëª© ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§")
async def crawl_all_major_stocks():
    """ì£¼ìš” ì¢…ëª©ë“¤ì˜ ë‰´ìŠ¤ë¥¼ ëª¨ë‘ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    try:
        major_stocks = [
            ("005930", "ì‚¼ì„±ì „ì"),
            ("000660", "SKí•˜ì´ë‹‰ìŠ¤"),
            ("035420", "NAVER"),
            ("005380", "í˜„ëŒ€ì°¨"),
            ("035720", "ì¹´ì¹´ì˜¤")
        ]
        
        results = []
        for stock_code, stock_name in major_stocks:
            try:
                result = await news_crawler.crawl_and_analyze_stock(stock_name, stock_code)
                results.append(result)
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"{stock_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results.append({
                    "stock_name": stock_name,
                    "stock_code": stock_code,
                    "error": str(e)
                })
        
        return {
            "success": True,
            "data": results,
            "message": f"ì£¼ìš” ì¢…ëª© {len(results)}ê°œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": "ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

@router.get("/sentiment/{stock_code}", summary="ì¢…ëª©ë³„ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ê²°ê³¼ ì¡°íšŒ")
async def get_stock_sentiment(stock_code: str):
    """íŠ¹ì • ì¢…ëª©ì˜ ìµœì‹  ë‰´ìŠ¤ ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        # ì‹¤ì œë¡œëŠ” DBì—ì„œ ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ì§€ë§Œ
        # ì„ì‹œë¡œ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ë°˜í™˜
        stock_name_map = {
            "005930": "ì‚¼ì„±ì „ì",
            "000660": "SKí•˜ì´ë‹‰ìŠ¤", 
            "035420": "NAVER"
        }
        
        stock_name = stock_name_map.get(stock_code, f"ì¢…ëª©{stock_code}")
        result = await news_crawler.crawl_and_analyze_stock(stock_name, stock_code)
        
        # ê°ì • ë¶„ì„ ìš”ì•½ë§Œ ë°˜í™˜
        return {
            "stock_code": stock_code,
            "stock_name": stock_name,
            "sentiment_summary": result.get("sentiment_summary", {}),
            "overall_sentiment": result.get("overall_sentiment", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "article_count": len(result.get("articles", [])),
            "last_updated": result.get("crawled_at")
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": "ê°ì • ë¶„ì„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
    
@router.get("/stocks/major", summary="ì£¼ìš” ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ")
async def get_major_stocks():
    """ì£¼ìš” í•œêµ­ ì£¼ì‹ ì¢…ëª© 30ê°œ ë°˜í™˜"""
    major_stocks = [
        {"symbol": "005930", "name": "ì‚¼ì„±ì „ì", "market": "KOSPI"},
        {"symbol": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤", "market": "KOSPI"},
        {"symbol": "035420", "name": "NAVER", "market": "KOSPI"},
        {"symbol": "005380", "name": "í˜„ëŒ€ì°¨", "market": "KOSPI"},
        {"symbol": "005490", "name": "POSCOí™€ë”©ìŠ¤", "market": "KOSPI"},
        {"symbol": "051910", "name": "LGí™”í•™", "market": "KOSPI"},
        {"symbol": "006400", "name": "ì‚¼ì„±SDI", "market": "KOSPI"},
        {"symbol": "035720", "name": "ì¹´ì¹´ì˜¤", "market": "KOSPI"},
        {"symbol": "105560", "name": "KBê¸ˆìœµ", "market": "KOSPI"},
        {"symbol": "055550", "name": "ì‹ í•œì§€ì£¼", "market": "KOSPI"},
        {"symbol": "096770", "name": "SKì´ë…¸ë² ì´ì…˜", "market": "KOSPI"},
        {"symbol": "000270", "name": "ê¸°ì•„", "market": "KOSPI"},
        {"symbol": "323410", "name": "ì¹´ì¹´ì˜¤ë±…í¬", "market": "KOSPI"},
        {"symbol": "373220", "name": "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "market": "KOSPI"},
        {"symbol": "207940", "name": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "market": "KOSPI"},
        {"symbol": "066570", "name": "LGì „ì", "market": "KOSPI"},
        {"symbol": "017670", "name": "SKí…”ë ˆì½¤", "market": "KOSPI"},
        {"symbol": "033780", "name": "KT&G", "market": "KOSPI"},
        {"symbol": "068270", "name": "ì…€íŠ¸ë¦¬ì˜¨", "market": "KOSPI"},
        {"symbol": "352820", "name": "í•˜ì´ë¸Œ", "market": "KOSPI"},
        {"symbol": "091990", "name": "ì…€íŠ¸ë¦¬ì˜¨í—¬ìŠ¤ì¼€ì–´", "market": "KOSDAQ"},
        {"symbol": "196170", "name": "ì•Œí…Œì˜¤ì  ", "market": "KOSDAQ"},
        {"symbol": "066970", "name": "ì—˜ì•¤ì—í”„", "market": "KOSDAQ"},
        {"symbol": "263750", "name": "í„ì–´ë¹„ìŠ¤", "market": "KOSDAQ"},
        {"symbol": "036570", "name": "ì—”ì”¨ì†Œí”„íŠ¸", "market": "KOSPI"},
        {"symbol": "018260", "name": "ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤", "market": "KOSPI"},
        {"symbol": "009150", "name": "ì‚¼ì„±ì „ê¸°", "market": "KOSPI"},
        {"symbol": "003550", "name": "LG", "market": "KOSPI"},
        {"symbol": "028260", "name": "ì‚¼ì„±ë¬¼ì‚°", "market": "KOSPI"},
        {"symbol": "003670", "name": "í¬ìŠ¤ì½”í“¨ì²˜ì— ", "market": "KOSPI"}
    ]
    
    return {"count": len(major_stocks), "data": major_stocks}

@router.get("/stocks/all", summary="ì „ì²´ ì¢…ëª© ì¡°íšŒ (ì„ì‹œ)")
async def get_all_stocks_temp():
    """ì„ì‹œë¡œ ì£¼ìš” ì¢…ëª©ì„ ì „ì²´ ì¢…ëª©ìœ¼ë¡œ ë°˜í™˜"""
    return await get_major_stocks()

@router.post("/crawl-and-analyze/")
async def crawl_and_analyze_news_selenium(
    background_tasks: BackgroundTasks,
    ticker: str = Query(..., description="ì¢…ëª© ì½”ë“œ")
):
    """íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ - ê°œì„ ëœ í¬ë¡¤ë§"""
    try:
        import aiohttp
        from bs4 import BeautifulSoup
        from urllib.parse import quote
        from datetime import datetime
        
        # ì¢…ëª© ì½”ë“œ â†’ ì¢…ëª©ëª… ë§¤í•‘
        stock_name_map = {
            "005930": "ì‚¼ì„±ì „ì",
            "000660": "SKí•˜ì´ë‹‰ìŠ¤", 
            "035420": "NAVER",
            "005380": "í˜„ëŒ€ì°¨",
            "005490": "POSCOí™€ë”©ìŠ¤",
            "051910": "LGí™”í•™",
            "035720": "ì¹´ì¹´ì˜¤",
            "323410": "ì¹´ì¹´ì˜¤ë±…í¬",
            "373220": "LGì—ë„ˆì§€ì†”ë£¨ì…˜"
        }
        
        stock_name = stock_name_map.get(ticker, f"ì¢…ëª©{ticker}")
        logger.info(f"ğŸ” {stock_name}({ticker}) ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        
        articles = []
        
        try:
            # ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            # ë” ì •í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬
            search_query = f"{stock_name}"
            encoded_query = quote(search_query)
            url = f"https://search.naver.com/search.naver?where=news&query={encoded_query}&sort=1"
            
            logger.info(f"ğŸ“¡ í¬ë¡¤ë§ URL: {url}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=15) as response:
                    logger.info(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status}")
                    
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ì—¬ëŸ¬ ì…€ë ‰í„°ë¡œ ë‰´ìŠ¤ ì°¾ê¸°
                        news_items = []
                        
                        # ë°©ë²• 1: news_tit í´ë˜ìŠ¤
                        news_titles_1 = soup.find_all('a', class_='news_tit')
                        if news_titles_1:
                            logger.info(f"âœ… news_titìœ¼ë¡œ {len(news_titles_1)}ê°œ ë°œê²¬")
                            news_items.extend(news_titles_1[:3])
                        
                        # ë°©ë²• 2: ë‹¤ë¥¸ ë‰´ìŠ¤ ì…€ë ‰í„°
                        if not news_items:
                            news_titles_2 = soup.find_all('a', attrs={'data-clk': True})
                            if news_titles_2:
                                logger.info(f"âœ… data-clkë¡œ {len(news_titles_2)}ê°œ ë°œê²¬")
                                news_items.extend([item for item in news_titles_2 if 'news' in str(item)][:3])
                        
                        # ë°©ë²• 3: ì œëª©ì´ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
                        if not news_items:
                            all_links = soup.find_all('a', href=True)
                            news_links = [link for link in all_links if stock_name in link.get_text()]
                            if news_links:
                                logger.info(f"âœ… í…ìŠ¤íŠ¸ ë§¤ì¹­ìœ¼ë¡œ {len(news_links)}ê°œ ë°œê²¬")
                                news_items.extend(news_links[:3])
                        
                        logger.info(f"ğŸ“° ì´ ì¶”ì¶œëœ ë‰´ìŠ¤ ì•„ì´í…œ: {len(news_items)}")
                        
                        # ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬
                        for i, item in enumerate(news_items[:5]):
                            try:
                                # ì œëª© ì¶”ì¶œ
                                title = item.get('title', '') or item.get_text(strip=True)
                                url_link = item.get('href', '')
                                
                                if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                                    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
                                    positive_keywords = ['ìƒìŠ¹', 'í˜¸ì¬', 'ì„±ì¥', 'ì¦ê°€', 'ê°œì„ ', 'í™•ëŒ€', 'íˆ¬ì', 'ê¸ì •', 'ê¸°ëŒ€']
                                    negative_keywords = ['í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ë¶€ì§„', 'ìš°ë ¤', 'ìœ„í—˜', 'ì†ì‹¤', 'ë¶€ì •']
                                    
                                    sentiment = "ì¤‘ë¦½"
                                    confidence = 0.6
                                    
                                    pos_count = sum(1 for kw in positive_keywords if kw in title)
                                    neg_count = sum(1 for kw in negative_keywords if kw in title)
                                    
                                    if pos_count > neg_count:
                                        sentiment = "ê¸ì •"
                                        confidence = min(0.9, 0.6 + pos_count * 0.1)
                                    elif neg_count > pos_count:
                                        sentiment = "ë¶€ì •"
                                        confidence = min(0.9, 0.6 + neg_count * 0.1)
                                    
                                    articles.append({
                                        'title': title,
                                        'url': url_link if url_link.startswith('http') else f"https://search.naver.com{url_link}",
                                        'source': 'ë„¤ì´ë²„ë‰´ìŠ¤',
                                        'time': 'ë°©ê¸ˆ ì „',
                                        'sentiment': sentiment,
                                        'confidence': confidence,
                                        'ai_analysis': {
                                            'investment_advice': "ë§¤ìˆ˜ ê²€í† " if sentiment == "ê¸ì •" else "ê´€ë§" if sentiment == "ì¤‘ë¦½" else "ì£¼ì˜",
                                            'price_impact': "ìƒìŠ¹ ì˜ˆìƒ" if sentiment == "ê¸ì •" else "ì¤‘ë¦½" if sentiment == "ì¤‘ë¦½" else "í•˜ë½ ìš°ë ¤",
                                            'summary': f"{stock_name} ê´€ë ¨ {sentiment}ì  ë‰´ìŠ¤ - {title[:50]}..."
                                        }
                                    })
                                    
                                    logger.info(f"âœ… ë‰´ìŠ¤ {i+1}: {title[:30]}... (ê°ì •: {sentiment})")
                                    
                            except Exception as item_error:
                                logger.warning(f"âš ï¸ ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨: {item_error}")
                                continue
                    
                    else:
                        logger.error(f"âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: {response.status}")
        
        except Exception as crawl_error:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_error}")
        
        # ì‹¤ì œ í¬ë¡¤ë§ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        if articles:
            logger.info(f"âœ… ì‹¤ì œ ë‰´ìŠ¤ {len(articles)}ê°œ í¬ë¡¤ë§ ì„±ê³µ")
        else:
            logger.warning(f"âš ï¸ ì‹¤ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„° ì œê³µ")
            # ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œ í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œì—ë§Œ)
            articles = [
                {
                    'title': f"[ì‹¤ì‹œê°„] {stock_name} ì£¼ê°€ ë™í–¥ ë° ì‹œì¥ ë¶„ì„",
                    'url': f"https://finance.naver.com/item/main.naver?code={ticker}",
                    'source': 'ë„¤ì´ë²„ê¸ˆìœµ',
                    'time': '1ì‹œê°„ ì „',
                    'sentiment': 'ì¤‘ë¦½',
                    'confidence': 0.7,
                    'ai_analysis': {
                        'investment_advice': "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§",
                        'price_impact': "ì§€ì† ê´€ì°° í•„ìš”",
                        'summary': f"{stock_name}ì˜ ì‹¤ì‹œê°„ ì£¼ê°€ ë™í–¥ì„ í™•ì¸í•´ë³´ì„¸ìš”."
                    }
                },
                {
                    'title': f"{stock_name} ê¸°ì—… ì •ë³´ ë° ì¬ë¬´ í˜„í™©",
                    'url': f"https://finance.naver.com/item/coinfo.naver?code={ticker}",
                    'source': 'ë„¤ì´ë²„ê¸ˆìœµ',
                    'time': '2ì‹œê°„ ì „',
                    'sentiment': 'ì¤‘ë¦½',
                    'confidence': 0.7,
                    'ai_analysis': {
                        'investment_advice': "ê¸°ë³¸ ë¶„ì„ ì°¸ê³ ",
                        'price_impact': "í€ë”ë©˜í„¸ í™•ì¸",
                        'summary': f"{stock_name}ì˜ ê¸°ì—… ì •ë³´ì™€ ì¬ë¬´ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    }
                }
            ]
        
        # ì „ì²´ ê°ì • ë¶„ì„ ìš”ì•½
        total_count = len(articles)
        positive_count = sum(1 for article in articles if article.get('sentiment') == 'ê¸ì •')
        negative_count = sum(1 for article in articles if article.get('sentiment') == 'ë¶€ì •')
        
        if positive_count > negative_count:
            overall_sentiment = "ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì "
            investment_recommendation = "ë§¤ìˆ˜ ê²€í†  ê¶Œì¥"
        elif negative_count > positive_count:
            overall_sentiment = "ì „ë°˜ì ìœ¼ë¡œ ë¶€ì •ì "
            investment_recommendation = "ì‹ ì¤‘í•œ ì ‘ê·¼ í•„ìš”"
        else:
            overall_sentiment = "ì¤‘ë¦½ì "
            investment_recommendation = "ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ í•„ìš”"
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "ticker": ticker,
            "stock_name": stock_name,
            "crawled_articles": total_count,
            "articles": articles,
            "ai_analysis_summary": {
                "overall_sentiment": overall_sentiment,
                "positive_ratio": round(positive_count/total_count, 2) if total_count > 0 else 0,
                "investment_recommendation": investment_recommendation,
                "market_outlook": f"{stock_name}ì— ëŒ€í•œ ë‰´ìŠ¤ ë¶„ìœ„ê¸°ëŠ” {overall_sentiment}ì…ë‹ˆë‹¤.",
                "key_insights": [
                    f"ì´ {total_count}ê°œ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ",
                    f"ê¸ì •ì  ë‰´ìŠ¤: {positive_count}ê°œ, ë¶€ì •ì  ë‰´ìŠ¤: {negative_count}ê°œ",
                    f"íˆ¬ì ì¶”ì²œ: {investment_recommendation}"
                ]
            },
            "crawled_at": datetime.now().isoformat(),
            "status": "success"
        }
        
        logger.info(f"ğŸ¯ {stock_name} ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: {overall_sentiment}")
        
        return {
            "success": True,
            "message": f"{stock_name}({ticker}) ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ",
            "data": result
        }
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "ticker": ticker,
            "error": str(e),
            "message": "ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
    # 3. í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥í•˜ê³ , AI ë¶„ì„ ëª©ë¡ ì¤€ë¹„ (ë¹„ë™ê¸°)
    articles_to_analyze = []
    for article_data in crawled_articles_data:
        check_query = "SELECT id FROM news_articles WHERE url = %s"
        if await execute_query(check_query, (article_data["url"],), pool=pool):
            continue

        date_str = article_data.get('date_str', '')
        published_dt = None
        try:
            published_dt = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
        except (ValueError, TypeError):
            try:
                published_dt = datetime.strptime(date_str, "%Y.%m.%d")
            except (ValueError, TypeError): pass
        
        insert_query = """
            INSERT INTO news_articles (stock_id, title, url, source, published_at, content_preview)
            VALUES (%s, %s, %s, %s, %s, %s)
        """
        params = (stock_info['id'], article_data['title'], article_data['url'], article_data['source'], published_dt, article_data['content_preview'])
        await execute_query(insert_query, params, pool=pool)

        get_new_query = "SELECT id FROM news_articles WHERE url = %s"
        new_article_list = await execute_query(get_new_query, (article_data['url'],), pool=pool)
        if new_article_list:
            article_data['id'] = new_article_list[0]['id']
            articles_to_analyze.append(article_data)

    # 4. AI ë¶„ì„ì„ ë™ì‹œì— ì‹¤í–‰ (ë¹„ë™ê¸°)
    analysis_tasks = [
        news_service.analyze_single_article(
            article_id=article['id'],
            article_title=article['title'],
            article_content=article['content_preview'],
            stock_info=stock_info,
            pool=pool
        ) for article in articles_to_analyze
    ]
    analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
    
    # 5. ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ë°˜í™˜
    successful_analyses = 0
    details = []
    for i, article in enumerate(articles_to_analyze):
        result = analysis_results[i]
        is_success = not isinstance(result, Exception) and result is not None
        
        if is_success:
            successful_analyses += 1
        
        details.append({
            "article_id": article['id'],
            "title": article['title'],
            "analysis_status": "ì„±ê³µ" if is_success else "ì‹¤íŒ¨",
            "sentiment": result.get('sentiment') if is_success else None
        })

    return {
        "message": f"'{stock_info['name']}' ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ.",
        "crawled_count": len(articles_to_analyze),
        "successful_analyses": successful_analyses,
        "failed_analyses": len(articles_to_analyze) - successful_analyses,
        "details": details
    }





@router.post("/gemini-analyze/{ticker}")
async def gemini_analyze_news(ticker: str):
    """ğŸ¤– ë‰´ìŠ¤ë³„ ë§ì¶¤ ë¶„ì„ - ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš© ë°˜ì˜"""
    try:
        stock_name_map = {
            "005930": "ì‚¼ì„±ì „ì",
            "000660": "SKí•˜ì´ë‹‰ìŠ¤", 
            "035420": "NAVER",
            "005380": "í˜„ëŒ€ì°¨",
            "035720": "ì¹´ì¹´ì˜¤",
            "051910": "LGí™”í•™",
            "373220": "LGì—ë„ˆì§€ì†”ë£¨ì…˜"
        }
        
        stock_name = stock_name_map.get(ticker, f"ì¢…ëª©{ticker}")
        logger.info(f"ğŸ¤– {stock_name} ë‰´ìŠ¤ë³„ ë§ì¶¤ ë¶„ì„ ì‹œì‘")
        
        # ì‹¤ì œ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ê°œë³„ ë¶„ì„
        def analyze_individual_news(news_title, news_content=""):
            """ê°œë³„ ë‰´ìŠ¤ ë¶„ì„ í•¨ìˆ˜"""
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ (ê°œì„ ëœ ë²„ì „)
            positive_keywords = ['ìƒìŠ¹', 'í˜¸ì¬', 'ì„±ì¥', 'ì¦ê°€', 'ê°œì„ ', 'í™•ëŒ€', 'íˆ¬ì', 'ê¸ì •', 'ê¸°ëŒ€', 'í˜ì‹ ', 'ì„±ê³µ', 'ë°œí‘œ', 'ê³„ì•½', 'í˜‘ë ¥']
            negative_keywords = ['í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ë¶€ì§„', 'ìš°ë ¤', 'ìœ„í—˜', 'ì†ì‹¤', 'ë¶€ì •', 'ì¤‘ë‹¨', 'ì—°ê¸°', 'ì·¨ì†Œ', 'ë¬¸ì œ', 'ë…¼ë€', 'ì¡°ì‚¬']
            
            title_lower = news_title.lower()
            content_lower = news_content.lower()
            combined_text = f"{title_lower} {content_lower}"
            
            pos_count = sum(1 for kw in positive_keywords if kw in combined_text)
            neg_count = sum(1 for kw in negative_keywords if kw in combined_text)
            
            # ê°ì • ì ìˆ˜ ê³„ì‚°
            if pos_count > neg_count:
                sentiment = "ê¸ì •"
                confidence = min(0.95, 0.6 + (pos_count - neg_count) * 0.1)
                investment_advice = "ë§¤ìˆ˜ ê²€í† "
                price_impact = "ìƒìŠ¹ ì˜ˆìƒ"
            elif neg_count > pos_count:
                sentiment = "ë¶€ì •"
                confidence = min(0.95, 0.6 + (neg_count - pos_count) * 0.1)
                investment_advice = "ì£¼ì˜ ê´€ì°°"
                price_impact = "í•˜ë½ ìš°ë ¤"
            else:
                sentiment = "ì¤‘ë¦½"
                confidence = 0.65
                investment_advice = "ê´€ë§"
                price_impact = "ë³€ë™ì„± ì˜ˆìƒ"
            
            # ë‰´ìŠ¤ ì œëª©ì— ë”°ë¥¸ ë§ì¶¤ ìš”ì•½ ìƒì„±
            if "ì‹¤ì " in news_title or "ìˆ˜ìµ" in news_title:
                summary = f"{stock_name}ì˜ ì‹¤ì  ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤. {sentiment}ì  ì „ë§ì´ ì œì‹œë˜ê³  ìˆì–´ ì£¼ê°€ì— {price_impact.split()[0]} ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
            elif "ê¸°ìˆ " in news_title or "ê°œë°œ" in news_title or "í˜ì‹ " in news_title:
                summary = f"{stock_name}ì˜ ê¸°ìˆ  ê°œë°œ ê´€ë ¨ ì†Œì‹ì…ë‹ˆë‹¤. í˜ì‹  ê¸°ìˆ ì— ëŒ€í•œ {sentiment}ì  í‰ê°€ë¡œ ì¥ê¸°ì ì¸ ì„±ì¥ ê°€ëŠ¥ì„±ì— {price_impact.split()[0]} ìš”ì¸ìœ¼ë¡œ ì‘ìš©í•  ì „ë§ì…ë‹ˆë‹¤."
            elif "íˆ¬ì" in news_title or "ê³„ì•½" in news_title:
                summary = f"{stock_name}ì˜ ì‚¬ì—… í™•ì¥ ë° íˆ¬ì ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì‚¬ì—… ê¸°íšŒì— ëŒ€í•œ {sentiment}ì  ì‹œê°ìœ¼ë¡œ í–¥í›„ {price_impact}ë©ë‹ˆë‹¤."
            elif "ì •ë¶€" in news_title or "ì •ì±…" in news_title:
                summary = f"{stock_name}ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì •ì±… ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤. ì •ì±… ë³€í™”ê°€ {sentiment}ì ìœ¼ë¡œ í•´ì„ë˜ì–´ {price_impact}ë¡œ ë¶„ì„ë©ë‹ˆë‹¤."
            elif "ê¸€ë¡œë²Œ" in news_title or "í•´ì™¸" in news_title:
                summary = f"{stock_name}ì˜ ê¸€ë¡œë²Œ ì‚¬ì—… ê´€ë ¨ ì†Œì‹ì…ë‹ˆë‹¤. í•´ì™¸ ì‹œì¥ ì „ë§ì´ {sentiment}ì ìœ¼ë¡œ í‰ê°€ë˜ì–´ {price_impact}ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
            else:
                # ê¸°ë³¸ ìš”ì•½ (ì œëª©ì˜ í•µì‹¬ í‚¤ì›Œë“œ í™œìš©)
                key_words = [word for word in news_title.split() if len(word) > 1][:3]
                summary = f"{stock_name} ê´€ë ¨ ë‰´ìŠ¤ - {', '.join(key_words)} ë“±ì˜ ì´ìŠˆë¡œ {sentiment}ì  ì‹œì¥ ë¶„ìœ„ê¸°ê°€ í˜•ì„±ë˜ì–´ {price_impact}ë¡œ ë¶„ì„ë©ë‹ˆë‹¤."
            
            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'investment_advice': investment_advice,
                'price_impact': price_impact,
                'summary': summary,
                'key_factors': [
                    f"ë‰´ìŠ¤ ê°ì •: {sentiment}",
                    f"íˆ¬ì ì¡°ì–¸: {investment_advice}",
                    f"ì˜ˆìƒ ì˜í–¥: {price_impact}"
                ]
            }
        
        # ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„° (ì‹¤ì œë¡œëŠ” í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ì‚¬ìš©)
        sample_news_titles = [
            f"{stock_name} 3ë¶„ê¸° ì‹¤ì  ê°œì„  ì „ë§, ì• ë„ë¦¬ìŠ¤íŠ¸ë“¤ ëª©í‘œê°€ ìƒí–¥ ì¡°ì •",
            f"{stock_name} ì°¨ì„¸ëŒ€ AI ê¸°ìˆ  ê°œë°œ ì„±ê³µ, ê¸€ë¡œë²Œ ì‹œì¥ ì§„ì¶œ ê°€ì†í™”",
            f"{stock_name} ëŒ€ê·œëª¨ ì‹ ê·œ íˆ¬ì ê³„íš ë°œí‘œ, ë¯¸ë˜ ì„±ì¥ ë™ë ¥ í™•ë³´"
        ]
        
        analyzed_articles = []
        for i, title in enumerate(sample_news_titles):
            analysis = analyze_individual_news(title)
            analyzed_articles.append({
                'title': title,
                'content': f"{title}ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì…ë‹ˆë‹¤.",
                'url': f"https://news.sample.com/{ticker}_{i}",
                'source': 'ë„¤ì´ë²„ë‰´ìŠ¤',
                'time': f"{i+1}ì‹œê°„ ì „",
                'gemini_analysis': analysis
            })
        
        # ì „ì²´ ë¶„ì„ ìš”ì•½
        positive_count = sum(1 for article in analyzed_articles if article['gemini_analysis']['sentiment'] == 'ê¸ì •')
        total_count = len(analyzed_articles)
        
        if positive_count > total_count / 2:
            overall_sentiment = "ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì "
            recommendation = "ë§¤ìˆ˜ ê²€í†  ê¶Œì¥"
        elif positive_count == 0:
            overall_sentiment = "ì „ë°˜ì ìœ¼ë¡œ ë¶€ì •ì "
            recommendation = "ì‹ ì¤‘í•œ ì ‘ê·¼ í•„ìš”"
        else:
            overall_sentiment = "ì¤‘ë¦½ì "
            recommendation = "ì¶”ê°€ ëª¨ë‹ˆí„°ë§ í•„ìš”"
        
        # ì¢…í•© ìš”ì•½ (ë‰´ìŠ¤ ë‚´ìš© ë°˜ì˜)
        news_themes = []
        if any("ì‹¤ì " in article['title'] for article in analyzed_articles):
            news_themes.append("ì‹¤ì  ê°œì„ ")
        if any("ê¸°ìˆ " in article['title'] or "ê°œë°œ" in article['title'] for article in analyzed_articles):
            news_themes.append("ê¸°ìˆ  í˜ì‹ ")
        if any("íˆ¬ì" in article['title'] for article in analyzed_articles):
            news_themes.append("ì‚¬ì—… í™•ì¥")
        
        theme_text = ", ".join(news_themes) if news_themes else "ë‹¤ì–‘í•œ ì´ìŠˆ"
        
        overall_summary = f"{stock_name}ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•œ ê²°ê³¼, {theme_text} ê´€ë ¨ ì†Œì‹ë“¤ì´ ì£¼ìš”í•˜ê²Œ ë‹¤ë¤„ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ {overall_sentiment} ë¶„ìœ„ê¸°ê°€ í˜•ì„±ë˜ì–´ ìˆì–´, {recommendation}í•˜ëŠ” ê²ƒì´ ì ì ˆí•´ ë³´ì…ë‹ˆë‹¤."
        
        return {
            "success": True,
            "ticker": ticker,
            "message": f"{stock_name} ë§ì¶¤í˜• ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ",
            "data": {
                "stock_name": stock_name,
                "stock_code": ticker,
                "articles": analyzed_articles,
                "gemini_analysis": {
                    "overall_sentiment": overall_sentiment,
                    "average_sentiment_score": round(positive_count/total_count, 2),
                    "total_articles": total_count,
                    "summary": overall_summary,
                    "recommendation": recommendation,
                    "key_insights": [
                        f"ë¶„ì„ëœ ì£¼ìš” í…Œë§ˆ: {theme_text}",
                        f"ê¸ì •ì  ë‰´ìŠ¤ ë¹„ìœ¨: {round(positive_count/total_count*100)}%",
                        f"íˆ¬ì ë°©í–¥: {recommendation}"
                    ]
                },
                "analysis_stats": {
                    "analyzed_at": datetime.now().isoformat(),
                    "analysis_method": "í‚¤ì›Œë“œ ê¸°ë°˜ + ë§ì¶¤í˜• ìš”ì•½"
                }
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }